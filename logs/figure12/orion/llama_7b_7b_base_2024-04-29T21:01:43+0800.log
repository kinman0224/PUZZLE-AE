2024-04-29 21:01:47,683	INFO worker.py:1540 -- Connecting to existing Ray cluster at address: 10.10.10.238:6379...
2024-04-29 21:01:47,690	INFO worker.py:1724 -- Connected to Ray cluster.
[36m(TimeSharedModelRayRole pid=7821)[0m You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[36m(TimeSharedModelRayRole pid=7974)[0m 2024-04-29 21:01:55 INFO     Added key: store_based_barrier_key:1 to store for rank: 1
[36m(TimeSharedModelRayRole pid=7974)[0m 2024-04-29 21:02:05 INFO     Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=16, worker_count=7, timeout=0:10:00)
[36m(TimeSharedModelRayRole pid=7978)[0m You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[36m(TimeSharedModelRayRole pid=7978)[0m 2024-04-29 21:01:55 INFO     Added key: store_based_barrier_key:1 to store for rank: 5[32m [repeated 6x across cluster][0m
[36m(TimeSharedModelRayRole pid=7974)[0m 2024-04-29 21:02:15 INFO     Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=16, worker_count=7, timeout=0:10:00)[32m [repeated 7x across cluster][0m
[36m(TimeSharedModelRayRole pid=7974)[0m 2024-04-29 21:02:25 INFO     Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=16, worker_count=7, timeout=0:10:00)[32m [repeated 7x across cluster][0m
[36m(TimeSharedModelRayRole pid=24084, ip=10.10.10.239)[0m You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[36m(TimeSharedModelRayRole pid=24084, ip=10.10.10.239)[0m 2024-04-29 21:02:27 INFO     Added key: store_based_barrier_key:1 to store for rank: 10
[36m(TimeSharedModelRayRole pid=24086, ip=10.10.10.239)[0m 2024-04-29 21:02:27 INFO     Rank 11: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
[36m(TimeSharedModelRayRole pid=7821)[0m Detected CUDA files, patching ldflags
[36m(TimeSharedModelRayRole pid=7821)[0m Emitting ninja build file /public/home/qinghuatest/ae/puzzle/megatron/fused_kernels/build/build.ninja...
[36m(TimeSharedModelRayRole pid=7821)[0m Building extension module scaled_upper_triang_masked_softmax_cuda...
[36m(TimeSharedModelRayRole pid=7821)[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[36m(TimeSharedModelRayRole pid=7821)[0m Loading extension module scaled_upper_triang_masked_softmax_cuda...
[36m(TimeSharedModelRayRole pid=7821)[0m Detected CUDA files, patching ldflags
[36m(TimeSharedModelRayRole pid=7821)[0m Emitting ninja build file /public/home/qinghuatest/ae/puzzle/megatron/fused_kernels/build/build.ninja...
[36m(TimeSharedModelRayRole pid=7821)[0m Building extension module scaled_masked_softmax_cuda...
[36m(TimeSharedModelRayRole pid=7821)[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[36m(TimeSharedModelRayRole pid=7821)[0m Loading extension module scaled_masked_softmax_cuda...
[36m(TimeSharedModelRayRole pid=7821)[0m Detected CUDA files, patching ldflags
[36m(TimeSharedModelRayRole pid=7821)[0m Emitting ninja build file /public/home/qinghuatest/ae/puzzle/megatron/fused_kernels/build/build.ninja...
[36m(TimeSharedModelRayRole pid=7821)[0m Building extension module scaled_softmax_cuda...
[36m(TimeSharedModelRayRole pid=7821)[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[36m(TimeSharedModelRayRole pid=7821)[0m Loading extension module scaled_softmax_cuda...
[36m(TimeSharedModelRayRole pid=7821)[0m {'tokenizer_type': 'PretrainedFromHF', 'tokenizer_model': '/public/thu_ljw_workspace/dataset/Llama-2-7b-hf/', 'data_path': ['/public/thu_ljw_workspace/dataset/Dahoas/rm-static/'], 'data_split': '2,4,4', 'data_output_path': '/tmp/data_files', 'seq_length': 512, 'actor_model_name_or_path': '/public/thu_ljw_workspace/dataset/Llama-2-7b-hf/', 'critic_model_name_or_path': '/public/thu_ljw_workspace/dataset/Llama-2-7b-hf/', 'max_prompt_seq_len': 256, 'max_answer_seq_len': 256, 'generation_batches': 1, 'ppo_epochs': 1, 'num_train_epochs': 1, 'gradient_accumulation_steps': 1, 'tensor_model_parallel_size': 1, 'pipeline_model_parallel_size': 16, 'use_contiguous_buffers_in_local_ddp': True, 'load_model_from_hf_config': True, 'load_model_hf_checkpoint': False, 'model_name_or_path': '/public/thu_ljw_workspace/dataset/Llama-2-7b-hf/', 'max_position_embeddings': 512, 'micro_batch_size': 4, 'global_batch_size': 256, 'inference_batch_times_seqlen_threshold': 1024, 'train_iters': 500000, 'add_bias_linear': False, 'add_position_embedding': False, 'lr': 0.00015, 'lr_decay_style': 'cosine', 'lr_decay_iters': 320000, 'min_lr': 1e-05, 'lr_warmup_fraction': 0.01, 'weight_decay': 0.01, 'clip_grad': 1.0, 'fp16': True, 'bf16': False, 'use_dis_bubble_generation': True, 'pf_stage_mbs': 4, 'ar_stage_mbs': 16, 'use_shadow': False, 'shadow_tensor_model_parallel_size': 1, 'shadow_pipeline_model_parallel_size': 1, 'bulk_switch_on': False, 'exp_repeat': 1, 'placement_type': 4}
[36m(TimeSharedModelRayRole pid=7821)[0m using world size: 16, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 16
[36m(TimeSharedModelRayRole pid=7821)[0m using torch.float16 for parameters ...
[36m(TimeSharedModelRayRole pid=7821)[0m ------------------------ arguments ------------------------
[36m(TimeSharedModelRayRole pid=7821)[0m   accumulate_allreduce_grads_in_fp32 .............. False
[36m(TimeSharedModelRayRole pid=7821)[0m   actor_model_name_or_path ........................ /public/thu_ljw_workspace/dataset/Llama-2-7b-hf/
[36m(TimeSharedModelRayRole pid=7821)[0m   adam_beta1 ...................................... 0.9
[36m(TimeSharedModelRayRole pid=7821)[0m   adam_beta2 ...................................... 0.999
[36m(TimeSharedModelRayRole pid=7821)[0m   adam_eps ........................................ 1e-08
[36m(TimeSharedModelRayRole pid=7821)[0m   add_bias_linear ................................. False
[36m(TimeSharedModelRayRole pid=7821)[0m   add_position_embedding .......................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   adlr_autoresume ................................. False
[36m(TimeSharedModelRayRole pid=7821)[0m   adlr_autoresume_interval ........................ 1000
[36m(TimeSharedModelRayRole pid=7821)[0m   apply_layernorm_1p .............................. False
[36m(TimeSharedModelRayRole pid=7821)[0m   apply_query_key_layer_scaling ................... True
[36m(TimeSharedModelRayRole pid=7821)[0m   apply_residual_connection_post_layernorm ........ False
[36m(TimeSharedModelRayRole pid=7821)[0m   ar_stage_mbs .................................... 16
[36m(TimeSharedModelRayRole pid=7821)[0m   async_tensor_model_parallel_allreduce ........... True
[36m(TimeSharedModelRayRole pid=7821)[0m   attention_dropout ............................... 0.1
[36m(TimeSharedModelRayRole pid=7821)[0m   attention_softmax_in_fp32 ....................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   barrier_with_L1_time ............................ True
[36m(TimeSharedModelRayRole pid=7821)[0m   bert_binary_head ................................ True
[36m(TimeSharedModelRayRole pid=7821)[0m   bert_embedder_type .............................. megatron
[36m(TimeSharedModelRayRole pid=7821)[0m   bert_load ....................................... None
[36m(TimeSharedModelRayRole pid=7821)[0m   bf16 ............................................ False
[36m(TimeSharedModelRayRole pid=7821)[0m   bias_dropout_fusion ............................. True
[36m(TimeSharedModelRayRole pid=7821)[0m   bias_gelu_fusion ................................ False
[36m(TimeSharedModelRayRole pid=7821)[0m   biencoder_projection_dim ........................ 0
[36m(TimeSharedModelRayRole pid=7821)[0m   biencoder_shared_query_context_model ............ False
[36m(TimeSharedModelRayRole pid=7821)[0m   block_data_path ................................. None
[36m(TimeSharedModelRayRole pid=7821)[0m   bulk_switch_on .................................. False
[36m(TimeSharedModelRayRole pid=7821)[0m   classes_fraction ................................ 1.0
[36m(TimeSharedModelRayRole pid=7821)[0m   clip_grad ....................................... 1.0
[36m(TimeSharedModelRayRole pid=7821)[0m   consumed_train_samples .......................... 0
[36m(TimeSharedModelRayRole pid=7821)[0m   consumed_valid_samples .......................... 0
[36m(TimeSharedModelRayRole pid=7821)[0m   critic_model_name_or_path ....................... /public/thu_ljw_workspace/dataset/Llama-2-7b-hf/
[36m(TimeSharedModelRayRole pid=7821)[0m   data_impl ....................................... infer
[36m(TimeSharedModelRayRole pid=7821)[0m   data_output_path ................................ /tmp/data_files
[36m(TimeSharedModelRayRole pid=7821)[0m   data_parallel_random_init ....................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   data_parallel_size .............................. 1
[36m(TimeSharedModelRayRole pid=7821)[0m   data_path ....................................... ['/public/thu_ljw_workspace/dataset/Dahoas/rm-static/']
[36m(TimeSharedModelRayRole pid=7821)[0m   data_per_class_fraction ......................... 1.0
[36m(TimeSharedModelRayRole pid=7821)[0m   data_sharding ................................... True
[36m(TimeSharedModelRayRole pid=7821)[0m   data_split ...................................... 2,4,4
[36m(TimeSharedModelRayRole pid=7821)[0m   dataloader_type ................................. single
[36m(TimeSharedModelRayRole pid=7821)[0m   DDP_impl ........................................ local
[36m(TimeSharedModelRayRole pid=7821)[0m   decoder_num_layers .............................. None
[36m(TimeSharedModelRayRole pid=7821)[0m   decoder_seq_length .............................. None
[36m(TimeSharedModelRayRole pid=7821)[0m   dino_bottleneck_size ............................ 256
[36m(TimeSharedModelRayRole pid=7821)[0m   dino_freeze_last_layer .......................... 1
[36m(TimeSharedModelRayRole pid=7821)[0m   dino_head_hidden_size ........................... 2048
[36m(TimeSharedModelRayRole pid=7821)[0m   dino_local_crops_number ......................... 10
[36m(TimeSharedModelRayRole pid=7821)[0m   dino_local_img_size ............................. 96
[36m(TimeSharedModelRayRole pid=7821)[0m   dino_norm_last_layer ............................ False
[36m(TimeSharedModelRayRole pid=7821)[0m   dino_teacher_temp ............................... 0.07
[36m(TimeSharedModelRayRole pid=7821)[0m   dino_warmup_teacher_temp ........................ 0.04
[36m(TimeSharedModelRayRole pid=7821)[0m   dino_warmup_teacher_temp_epochs ................. 30
[36m(TimeSharedModelRayRole pid=7821)[0m   distribute_saved_activations .................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   distributed_backend ............................. nccl
[36m(TimeSharedModelRayRole pid=7821)[0m   distributed_timeout_minutes ..................... 10
[36m(TimeSharedModelRayRole pid=7821)[0m   embedding_path .................................. None
[36m(TimeSharedModelRayRole pid=7821)[0m   empty_unused_memory_level ....................... 0
[36m(TimeSharedModelRayRole pid=7821)[0m   encoder_num_layers .............................. 32
[36m(TimeSharedModelRayRole pid=7821)[0m   encoder_seq_length .............................. 512
[36m(TimeSharedModelRayRole pid=7821)[0m   end_weight_decay ................................ 0.01
[36m(TimeSharedModelRayRole pid=7821)[0m   eod_mask_loss ................................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   eval_interval ................................... 1000
[36m(TimeSharedModelRayRole pid=7821)[0m   eval_iters ...................................... 100
[36m(TimeSharedModelRayRole pid=7821)[0m   evidence_data_path .............................. None
[36m(TimeSharedModelRayRole pid=7821)[0m   exit_duration_in_mins ........................... None
[36m(TimeSharedModelRayRole pid=7821)[0m   exit_interval ................................... None
[36m(TimeSharedModelRayRole pid=7821)[0m   exit_on_missing_checkpoint ...................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   exit_signal_handler ............................. False
[36m(TimeSharedModelRayRole pid=7821)[0m   exp_repeat ...................................... 1
[36m(TimeSharedModelRayRole pid=7821)[0m   ffn_hidden_size ................................. 11008
[36m(TimeSharedModelRayRole pid=7821)[0m   finetune ........................................ False
[36m(TimeSharedModelRayRole pid=7821)[0m   fp16 ............................................ True
[36m(TimeSharedModelRayRole pid=7821)[0m   fp16_lm_cross_entropy ........................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   fp32_residual_connection ........................ False
[36m(TimeSharedModelRayRole pid=7821)[0m   fp8_amax_compute_algo ........................... most_recent
[36m(TimeSharedModelRayRole pid=7821)[0m   fp8_amax_history_len ............................ 1
[36m(TimeSharedModelRayRole pid=7821)[0m   fp8_e4m3 ........................................ False
[36m(TimeSharedModelRayRole pid=7821)[0m   fp8_hybrid ...................................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   fp8_interval .................................... 1
[36m(TimeSharedModelRayRole pid=7821)[0m   fp8_margin ...................................... 0
[36m(TimeSharedModelRayRole pid=7821)[0m   fp8_wgrad ....................................... True
[36m(TimeSharedModelRayRole pid=7821)[0m   generation_batches .............................. 1
[36m(TimeSharedModelRayRole pid=7821)[0m   global_batch_size ............................... 256
[36m(TimeSharedModelRayRole pid=7821)[0m   gradient_accumulation_fusion .................... True
[36m(TimeSharedModelRayRole pid=7821)[0m   gradient_accumulation_steps ..................... 1
[36m(TimeSharedModelRayRole pid=7821)[0m   head_lr_mult .................................... 1.0
[36m(TimeSharedModelRayRole pid=7821)[0m   hidden_dropout .................................. 0.1
[36m(TimeSharedModelRayRole pid=7821)[0m   hidden_size ..................................... 4096
[36m(TimeSharedModelRayRole pid=7821)[0m   hysteresis ...................................... 2
[36m(TimeSharedModelRayRole pid=7821)[0m   ict_head_size ................................... None
[36m(TimeSharedModelRayRole pid=7821)[0m   ict_load ........................................ None
[36m(TimeSharedModelRayRole pid=7821)[0m   img_h ........................................... 224
[36m(TimeSharedModelRayRole pid=7821)[0m   img_w ........................................... 224
[36m(TimeSharedModelRayRole pid=7821)[0m   indexer_batch_size .............................. 128
[36m(TimeSharedModelRayRole pid=7821)[0m   indexer_log_interval ............................ 1000
[36m(TimeSharedModelRayRole pid=7821)[0m   inference_batch_times_seqlen_threshold .......... 1024
[36m(TimeSharedModelRayRole pid=7821)[0m   init_method_std ................................. 0.02
[36m(TimeSharedModelRayRole pid=7821)[0m   init_method_xavier_uniform ...................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   initial_loss_scale .............................. 4294967296
[36m(TimeSharedModelRayRole pid=7821)[0m   iter_per_epoch .................................. 1250
[36m(TimeSharedModelRayRole pid=7821)[0m   kv_channels ..................................... 128
[36m(TimeSharedModelRayRole pid=7821)[0m   layernorm_epsilon ............................... 1e-05
[36m(TimeSharedModelRayRole pid=7821)[0m   lazy_mpu_init ................................... None
[36m(TimeSharedModelRayRole pid=7821)[0m   load ............................................ None
[36m(TimeSharedModelRayRole pid=7821)[0m   load_model_from_hf_config ....................... True
[36m(TimeSharedModelRayRole pid=7821)[0m   load_model_hf_checkpoint ........................ False
[36m(TimeSharedModelRayRole pid=7821)[0m   local_rank ...................................... None
[36m(TimeSharedModelRayRole pid=7821)[0m   log_batch_size_to_tensorboard ................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   log_interval .................................... 100
[36m(TimeSharedModelRayRole pid=7821)[0m   log_learning_rate_to_tensorboard ................ True
[36m(TimeSharedModelRayRole pid=7821)[0m   log_loss_scale_to_tensorboard ................... True
[36m(TimeSharedModelRayRole pid=7821)[0m   log_memory_to_tensorboard ....................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   log_num_zeros_in_grad ........................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   log_params_norm ................................. False
[36m(TimeSharedModelRayRole pid=7821)[0m   log_timers_to_tensorboard ....................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   log_validation_ppl_to_tensorboard ............... False
[36m(TimeSharedModelRayRole pid=7821)[0m   log_world_size_to_tensorboard ................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   loss_scale ...................................... None
[36m(TimeSharedModelRayRole pid=7821)[0m   loss_scale_window ............................... 1000
[36m(TimeSharedModelRayRole pid=7821)[0m   lr .............................................. 0.00015
[36m(TimeSharedModelRayRole pid=7821)[0m   lr_decay_iters .................................. 320000
[36m(TimeSharedModelRayRole pid=7821)[0m   lr_decay_samples ................................ None
[36m(TimeSharedModelRayRole pid=7821)[0m   lr_decay_style .................................. cosine
[36m(TimeSharedModelRayRole pid=7821)[0m   lr_warmup_fraction .............................. 0.01
[36m(TimeSharedModelRayRole pid=7821)[0m   lr_warmup_iters ................................. 0
[36m(TimeSharedModelRayRole pid=7821)[0m   lr_warmup_samples ............................... 0
[36m(TimeSharedModelRayRole pid=7821)[0m   make_vocab_size_divisible_by .................... 128
[36m(TimeSharedModelRayRole pid=7821)[0m   mask_factor ..................................... 1.0
[36m(TimeSharedModelRayRole pid=7821)[0m   mask_prob ....................................... 0.15
[36m(TimeSharedModelRayRole pid=7821)[0m   mask_type ....................................... random
[36m(TimeSharedModelRayRole pid=7821)[0m   masked_softmax_fusion ........................... True
[36m(TimeSharedModelRayRole pid=7821)[0m   max_answer_seq_len .............................. 256
[36m(TimeSharedModelRayRole pid=7821)[0m   max_position_embeddings ......................... 512
[36m(TimeSharedModelRayRole pid=7821)[0m   max_prompt_seq_len .............................. 256
[36m(TimeSharedModelRayRole pid=7821)[0m   max_tokens_to_oom ............................... 12000
[36m(TimeSharedModelRayRole pid=7821)[0m   merge_file ...................................... None
[36m(TimeSharedModelRayRole pid=7821)[0m   micro_batch_size ................................ 4
[36m(TimeSharedModelRayRole pid=7821)[0m   min_loss_scale .................................. 1.0
[36m(TimeSharedModelRayRole pid=7821)[0m   min_lr .......................................... 1e-05
[36m(TimeSharedModelRayRole pid=7821)[0m   mmap_warmup ..................................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   model_name_or_path .............................. /public/thu_ljw_workspace/dataset/Llama-2-7b-hf/
[36m(TimeSharedModelRayRole pid=7821)[0m   no_load_optim ................................... None
[36m(TimeSharedModelRayRole pid=7821)[0m   no_load_rng ..................................... None
[36m(TimeSharedModelRayRole pid=7821)[0m   no_persist_layer_norm ........................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   no_save_optim ................................... None
[36m(TimeSharedModelRayRole pid=7821)[0m   no_save_rng ..................................... None
[36m(TimeSharedModelRayRole pid=7821)[0m   num_attention_heads ............................. 32
[36m(TimeSharedModelRayRole pid=7821)[0m   num_channels .................................... 3
[36m(TimeSharedModelRayRole pid=7821)[0m   num_classes ..................................... 1000
[36m(TimeSharedModelRayRole pid=7821)[0m   num_experts ..................................... None
[36m(TimeSharedModelRayRole pid=7821)[0m   num_layers ...................................... 32
[36m(TimeSharedModelRayRole pid=7821)[0m   num_layers_per_virtual_pipeline_stage ........... None
[36m(TimeSharedModelRayRole pid=7821)[0m   num_train_epochs ................................ 1
[36m(TimeSharedModelRayRole pid=7821)[0m   num_workers ..................................... 2
[36m(TimeSharedModelRayRole pid=7821)[0m   onnx_safe ....................................... None
[36m(TimeSharedModelRayRole pid=7821)[0m   openai_gelu ..................................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   optimizer ....................................... adam
[36m(TimeSharedModelRayRole pid=7821)[0m   output_bert_embeddings .......................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   override_opt_param_scheduler .................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   params_dtype .................................... torch.float16
[36m(TimeSharedModelRayRole pid=7821)[0m   patch_dim ....................................... 16
[36m(TimeSharedModelRayRole pid=7821)[0m   perform_initialization .......................... True
[36m(TimeSharedModelRayRole pid=7821)[0m   pf_stage_mbs .................................... 4
[36m(TimeSharedModelRayRole pid=7821)[0m   pipeline_model_parallel_size .................... 16
[36m(TimeSharedModelRayRole pid=7821)[0m   pipeline_model_parallel_split_rank .............. None
[36m(TimeSharedModelRayRole pid=7821)[0m   placement_type .................................. 4
[36m(TimeSharedModelRayRole pid=7821)[0m   ppo_epochs ...................................... 1
[36m(TimeSharedModelRayRole pid=7821)[0m   query_in_block_prob ............................. 0.1
[36m(TimeSharedModelRayRole pid=7821)[0m   rampup_batch_size ............................... None
[36m(TimeSharedModelRayRole pid=7821)[0m   rank ............................................ 0
[36m(TimeSharedModelRayRole pid=7821)[0m   recompute_granularity ........................... None
[36m(TimeSharedModelRayRole pid=7821)[0m   recompute_method ................................ None
[36m(TimeSharedModelRayRole pid=7821)[0m   recompute_num_layers ............................ 1
[36m(TimeSharedModelRayRole pid=7821)[0m   reset_attention_mask ............................ False
[36m(TimeSharedModelRayRole pid=7821)[0m   reset_position_ids .............................. False
[36m(TimeSharedModelRayRole pid=7821)[0m   retriever_report_topk_accuracies ................ []
[36m(TimeSharedModelRayRole pid=7821)[0m   retriever_score_scaling ......................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   retriever_seq_length ............................ 256
[36m(TimeSharedModelRayRole pid=7821)[0m   retro_add_retriever ............................. False
[36m(TimeSharedModelRayRole pid=7821)[0m   retro_cyclic_train_iters ........................ None
[36m(TimeSharedModelRayRole pid=7821)[0m   retro_encoder_attention_dropout ................. 0.1
[36m(TimeSharedModelRayRole pid=7821)[0m   retro_encoder_hidden_dropout .................... 0.1
[36m(TimeSharedModelRayRole pid=7821)[0m   retro_encoder_layers ............................ 2
[36m(TimeSharedModelRayRole pid=7821)[0m   retro_num_neighbors ............................. 2
[36m(TimeSharedModelRayRole pid=7821)[0m   retro_num_retrieved_chunks ...................... 2
[36m(TimeSharedModelRayRole pid=7821)[0m   retro_return_doc_ids ............................ False
[36m(TimeSharedModelRayRole pid=7821)[0m   retro_workdir ................................... None
[36m(TimeSharedModelRayRole pid=7821)[0m   rotary_percent .................................. 1.0
[36m(TimeSharedModelRayRole pid=7821)[0m   sample_rate ..................................... 1.0
[36m(TimeSharedModelRayRole pid=7821)[0m   save ............................................ None
[36m(TimeSharedModelRayRole pid=7821)[0m   save_interval ................................... None
[36m(TimeSharedModelRayRole pid=7821)[0m   scatter_gather_tensors_in_pipeline .............. True
[36m(TimeSharedModelRayRole pid=7821)[0m   seed ............................................ 1234
[36m(TimeSharedModelRayRole pid=7821)[0m   seq_length ...................................... 512
[36m(TimeSharedModelRayRole pid=7821)[0m   sequence_parallel ............................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   sgd_momentum .................................... 0.9
[36m(TimeSharedModelRayRole pid=7821)[0m   shadow_pipeline_model_parallel_size ............. 1
[36m(TimeSharedModelRayRole pid=7821)[0m   shadow_tensor_model_parallel_size ............... 1
[36m(TimeSharedModelRayRole pid=7821)[0m   short_seq_prob .................................. 0.1
[36m(TimeSharedModelRayRole pid=7821)[0m   split ........................................... 969, 30, 1
[36m(TimeSharedModelRayRole pid=7821)[0m   squared_relu .................................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   standalone_embedding_stage ...................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   start_weight_decay .............................. 0.01
[36m(TimeSharedModelRayRole pid=7821)[0m   swiglu .......................................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   swin_backbone_type .............................. tiny
[36m(TimeSharedModelRayRole pid=7821)[0m   tensor_model_parallel_size ...................... 1
[36m(TimeSharedModelRayRole pid=7821)[0m   tensorboard_dir ................................. None
[36m(TimeSharedModelRayRole pid=7821)[0m   tensorboard_log_interval ........................ 1
[36m(TimeSharedModelRayRole pid=7821)[0m   tensorboard_queue_size .......................... 1000
[36m(TimeSharedModelRayRole pid=7821)[0m   test_data_path .................................. None
[36m(TimeSharedModelRayRole pid=7821)[0m   timing_log_level ................................ 0
[36m(TimeSharedModelRayRole pid=7821)[0m   timing_log_option ............................... minmax
[36m(TimeSharedModelRayRole pid=7821)[0m   titles_data_path ................................ None
[36m(TimeSharedModelRayRole pid=7821)[0m   tokenizer_model ................................. /public/thu_ljw_workspace/dataset/Llama-2-7b-hf/
[36m(TimeSharedModelRayRole pid=7821)[0m   tokenizer_type .................................. PretrainedFromHF
[36m(TimeSharedModelRayRole pid=7821)[0m   train_data_path ................................. None
[36m(TimeSharedModelRayRole pid=7821)[0m   train_iters ..................................... 500000
[36m(TimeSharedModelRayRole pid=7821)[0m   train_samples ................................... None
[36m(TimeSharedModelRayRole pid=7821)[0m   transformer_impl ................................ local
[36m(TimeSharedModelRayRole pid=7821)[0m   transformer_pipeline_model_parallel_size ........ 16
[36m(TimeSharedModelRayRole pid=7821)[0m   untie_embeddings_and_output_weights ............. False
[36m(TimeSharedModelRayRole pid=7821)[0m   use_checkpoint_args ............................. False
[36m(TimeSharedModelRayRole pid=7821)[0m   use_checkpoint_opt_param_scheduler .............. False
[36m(TimeSharedModelRayRole pid=7821)[0m   use_contiguous_buffers_in_local_ddp ............. True
[36m(TimeSharedModelRayRole pid=7821)[0m   use_cpu_initialization .......................... None
[36m(TimeSharedModelRayRole pid=7821)[0m   use_dis_bubble_generation ....................... True
[36m(TimeSharedModelRayRole pid=7821)[0m   use_distributed_optimizer ....................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   use_flash_attn .................................. False
[36m(TimeSharedModelRayRole pid=7821)[0m   use_one_sent_docs ............................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   use_ring_exchange_p2p ........................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   use_rotary_position_embeddings .................. False
[36m(TimeSharedModelRayRole pid=7821)[0m   use_shadow ...................................... False
[36m(TimeSharedModelRayRole pid=7821)[0m   valid_data_path ................................. None
[36m(TimeSharedModelRayRole pid=7821)[0m   variable_seq_lengths ............................ False
[36m(TimeSharedModelRayRole pid=7821)[0m   virtual_pipeline_model_parallel_size ............ None
[36m(TimeSharedModelRayRole pid=7821)[0m   vision_backbone_type ............................ vit
[36m(TimeSharedModelRayRole pid=7821)[0m   vision_pretraining .............................. False
[36m(TimeSharedModelRayRole pid=7821)[0m   vision_pretraining_type ......................... classify
[36m(TimeSharedModelRayRole pid=7821)[0m   vocab_extra_ids ................................. 0
[36m(TimeSharedModelRayRole pid=7821)[0m   vocab_file ...................................... None
[36m(TimeSharedModelRayRole pid=7821)[0m   vocab_size ...................................... None
[36m(TimeSharedModelRayRole pid=7821)[0m   weight_decay .................................... 0.01
[36m(TimeSharedModelRayRole pid=7821)[0m   weight_decay_incr_style ......................... constant
[36m(TimeSharedModelRayRole pid=7821)[0m   world_size ...................................... 16
[36m(TimeSharedModelRayRole pid=7821)[0m -------------------- end of arguments ---------------------
[36m(TimeSharedModelRayRole pid=7821)[0m setting number of micro-batches to constant 64
[36m(TimeSharedModelRayRole pid=7821)[0m > building PretrainedFromHF tokenizer ...
[36m(TimeSharedModelRayRole pid=7821)[0m  Loading tokenizer from pre-trained model
[36m(TimeSharedModelRayRole pid=7821)[0m  > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
[36m(TimeSharedModelRayRole pid=7821)[0m > initializing torch distributed ...
[36m(TimeSharedModelRayRole pid=24084, ip=10.10.10.239)[0m {'tokenizer_type': 'PretrainedFromHF', 'tokenizer_model': '/public/thu_ljw_workspace/dataset/Llama-2-7b-hf/', 'data_path': ['/public/thu_ljw_workspace/dataset/Dahoas/rm-static/'], 'data_split': '2,4,4', 'data_output_path': '/tmp/data_files', 'seq_length': 512, 'actor_model_name_or_path': '/public/thu_ljw_workspace/dataset/Llama-2-7b-hf/', 'critic_model_name_or_path': '/public/thu_ljw_workspace/dataset/Llama-2-7b-hf/', 'max_prompt_seq_len': 256, 'max_answer_seq_len': 256, 'generation_batches': 1, 'ppo_epochs': 1, 'num_train_epochs': 1, 'gradient_accumulation_steps': 1, 'tensor_model_parallel_size': 1, 'pipeline_model_parallel_size': 16, 'use_contiguous_buffers_in_local_ddp': True, 'load_model_from_hf_config': True, 'load_model_hf_checkpoint': False, 'model_name_or_path': '/public/thu_ljw_workspace/dataset/Llama-2-7b-hf/', 'max_position_embeddings': 512, 'micro_batch_size': 4, 'global_batch_size': 256, 'inference_batch_times_seqlen_threshold': 1024, 'train_iters': 500000, 'add_bias_linear': False, 'add_position_embedding': False, 'lr': 0.00015, 'lr_decay_style': 'cosine', 'lr_decay_iters': 320000, 'min_lr': 1e-05, 'lr_warmup_fraction': 0.01, 'weight_decay': 0.01, 'clip_grad': 1.0, 'fp16': True, 'bf16': False, 'use_dis_bubble_generation': True, 'pf_stage_mbs': 4, 'ar_stage_mbs': 16, 'use_shadow': False, 'shadow_tensor_model_parallel_size': 1, 'shadow_pipeline_model_parallel_size': 1, 'bulk_switch_on': False, 'exp_repeat': 1, 'placement_type': 4}[32m [repeated 8x across cluster][0m
[36m(TimeSharedModelRayRole pid=7821)[0m > initialized tensor model parallel with size 1
[36m(TimeSharedModelRayRole pid=7821)[0m > initialized pipeline model parallel with size 16
[36m(TimeSharedModelRayRole pid=7821)[0m > setting random seeds to 1234 ...
[36m(TimeSharedModelRayRole pid=7821)[0m > compiling dataset index builder ...
[36m(TimeSharedModelRayRole pid=7821)[0m make: Entering directory `/public/home/qinghuatest/ae/puzzle/megatron/data'
[36m(TimeSharedModelRayRole pid=7821)[0m make: Nothing to be done for `default'.
[36m(TimeSharedModelRayRole pid=7821)[0m make: Leaving directory `/public/home/qinghuatest/ae/puzzle/megatron/data'
[36m(TimeSharedModelRayRole pid=7821)[0m >>> done with dataset index builder. Compilation time: 0.051 seconds
[36m(TimeSharedModelRayRole pid=7821)[0m > compiling and loading fused kernels ...
[36m(TimeSharedModelRayRole pid=7821)[0m ninja: no work to do.
[36m(TimeSharedModelRayRole pid=7821)[0m ninja: no work to do.
[36m(TimeSharedModelRayRole pid=7821)[0m ninja: no work to do.
[36m(TimeSharedModelRayRole pid=7821)[0m >>> done with compiling and loading fused kernels. Compilation time: 5.899 seconds
[36m(TimeSharedModelRayRole pid=24090, ip=10.10.10.239)[0m {'tokenizer_type': 'PretrainedFromHF', 'tokenizer_model': '/public/thu_ljw_workspace/dataset/Llama-2-7b-hf/', 'data_path': ['/public/thu_ljw_workspace/dataset/Dahoas/rm-static/'], 'data_split': '2,4,4', 'data_output_path': '/tmp/data_files', 'seq_length': 512, 'actor_model_name_or_path': '/public/thu_ljw_workspace/dataset/Llama-2-7b-hf/', 'critic_model_name_or_path': '/public/thu_ljw_workspace/dataset/Llama-2-7b-hf/', 'max_prompt_seq_len': 256, 'max_answer_seq_len': 256, 'generation_batches': 1, 'ppo_epochs': 1, 'num_train_epochs': 1, 'gradient_accumulation_steps': 1, 'tensor_model_parallel_size': 1, 'pipeline_model_parallel_size': 16, 'use_contiguous_buffers_in_local_ddp': True, 'load_model_from_hf_config': True, 'load_model_hf_checkpoint': False, 'model_name_or_path': '/public/thu_ljw_workspace/dataset/Llama-2-7b-hf/', 'max_position_embeddings': 512, 'micro_batch_size': 4, 'global_batch_size': 256, 'inference_batch_times_seqlen_threshold': 1024, 'train_iters': 500000, 'add_bias_linear': False, 'add_position_embedding': False, 'lr': 0.00015, 'lr_decay_style': 'cosine', 'lr_decay_iters': 320000, 'min_lr': 1e-05, 'lr_warmup_fraction': 0.01, 'weight_decay': 0.01, 'clip_grad': 1.0, 'fp16': True, 'bf16': False, 'use_dis_bubble_generation': True, 'pf_stage_mbs': 4, 'ar_stage_mbs': 16, 'use_shadow': False, 'shadow_tensor_model_parallel_size': 1, 'shadow_pipeline_model_parallel_size': 1, 'bulk_switch_on': False, 'exp_repeat': 1, 'placement_type': 4}[32m [repeated 7x across cluster][0m
[36m(TimeSharedModelRayRole pid=24084, ip=10.10.10.239)[0m Creating the dataset ...
[36m(TimeSharedModelRayRole pid=24084, ip=10.10.10.239)[0m dataset_name: /public/thu_ljw_workspace/dataset/Dahoas/rm-static/
[36m(TimeSharedModelRayRole pid=7977)[0m Saving the dataset to the cache ...
[36m(TimeSharedModelRayRole pid=7979)[0m Creating the dataset ...[32m [repeated 15x across cluster][0m
[36m(TimeSharedModelRayRole pid=7979)[0m dataset_name: /public/thu_ljw_workspace/dataset/Dahoas/rm-static/[32m [repeated 15x across cluster][0m
[36m(TimeSharedModelRayRole pid=24083, ip=10.10.10.239)[0m  > number of parameters on (tensor, pipeline) model parallel rank (0, 8): 404766720
[36m(TimeSharedModelRayRole pid=24083, ip=10.10.10.239)[0m  > number of parameters on (tensor, pipeline) model parallel rank (0, 8): 404766720
[36m(TimeSharedModelRayRole pid=24083, ip=10.10.10.239)[0m  > number of parameters on (tensor, pipeline) model parallel rank (0, 8): 404766720
[36m(TimeSharedModelRayRole pid=24083, ip=10.10.10.239)[0m  > number of parameters on (tensor, pipeline) model parallel rank (0, 8): 404766720
[36m(TimeSharedModelRayRole pid=7978)[0m Saving the dataset to the cache ...[32m [repeated 15x across cluster][0m
num_total_iters: 239
Beginning of Epoch 1/1, Total Generation Batches 239
[36m(TimeSharedModelRayRole pid=7821)[0m building Llama model ...
[36m(TimeSharedModelRayRole pid=7821)[0m > learning rate decay style: cosine
[36m(TimeSharedModelRayRole pid=7821)[0m building Llama model ...
[36m(TimeSharedModelRayRole pid=7821)[0m > learning rate decay style: cosine
[36m(TimeSharedModelRayRole pid=7821)[0m building Llama model ...
[36m(TimeSharedModelRayRole pid=7821)[0m building Llama model ...
epoch: 0 | step: 0/239 | ppo_ep: 1 | global_batch_size: 256 | average_reward: -0.408 | act_loss: 5.840 | cri_loss: 6.866 | stage 1(gen.) (ms): 26045.438 | stage 2(infer.) (ms): 4377.967 | stage 3(train.) (ms): 11622.719 | e2e_time (ms): 42046.123
epoch: 0 | step: 1/239 | ppo_ep: 1 | global_batch_size: 256 | average_reward: -0.464 | act_loss: 5.757 | cri_loss: 6.825 | stage 1(gen.) (ms): 12326.504 | stage 2(infer.) (ms): 3048.822 | stage 3(train.) (ms): 8429.403 | e2e_time (ms): 23804.729
epoch: 0 | step: 2/239 | ppo_ep: 1 | global_batch_size: 256 | average_reward: -0.383 | act_loss: 5.761 | cri_loss: 6.864 | stage 1(gen.) (ms): 12344.808 | stage 2(infer.) (ms): 3059.106 | stage 3(train.) (ms): 8438.222 | e2e_time (ms): 23842.136
epoch: 0 | step: 3/239 | ppo_ep: 1 | global_batch_size: 256 | average_reward: -0.392 | act_loss: 5.634 | cri_loss: 6.789 | stage 1(gen.) (ms): 12526.385 | stage 2(infer.) (ms): 3058.024 | stage 3(train.) (ms): 8410.851 | e2e_time (ms): 23995.259
epoch: 0 | step: 4/239 | ppo_ep: 1 | global_batch_size: 256 | average_reward: -0.429 | act_loss: 5.676 | cri_loss: 6.941 | stage 1(gen.) (ms): 12154.852 | stage 2(infer.) (ms): 3070.067 | stage 3(train.) (ms): 8458.993 | e2e_time (ms): 23683.912
epoch: 0 | step: 5/239 | ppo_ep: 1 | global_batch_size: 256 | average_reward: -0.337 | act_loss: 5.536 | cri_loss: 6.885 | stage 1(gen.) (ms): 12409.019 | stage 2(infer.) (ms): 3082.396 | stage 3(train.) (ms): 8462.794 | e2e_time (ms): 23954.209
exit with early finished, for debug
[36m(TimeSharedModelRayRole pid=7978)[0m 2024-04-29 21:02:25 INFO     Waiting in store based barrier to initialize process group for rank: 5, key: store_based_barrier_key:1 (world_size=16, worker_count=7, timeout=0:10:00)[32m [repeated 6x across cluster][0m
[36m(TimeSharedModelRayRole pid=24090, ip=10.10.10.239)[0m You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565[32m [repeated 7x across cluster][0m
[36m(TimeSharedModelRayRole pid=24090, ip=10.10.10.239)[0m 2024-04-29 21:02:27 INFO     Added key: store_based_barrier_key:37 to store for rank: 15[32m [repeated 584x across cluster][0m
[36m(TimeSharedModelRayRole pid=7974)[0m 2024-04-29 21:02:27 INFO     Rank 1: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.[32m [repeated 591x across cluster][0m
[36m(TimeSharedModelRayRole pid=24090, ip=10.10.10.239)[0m  > number of parameters on (tensor, pipeline) model parallel rank (0, 15): 404774912[32m [repeated 60x across cluster][0m
